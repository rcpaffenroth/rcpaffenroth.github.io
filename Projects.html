<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Randy Paffenroth</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/small-business.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="http://wpi.edu">
                    <img src="images/WPI_Inst_Prim_FulClr_Rev_small.png" alt="">
                </a>
            </div>
            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
							<ul class="nav navbar-nav">
									<li>
											<a href="index.html">Home</a>
									</li>
                  <li>
                      <a href="Projects.html">
                        <img src="images/new.png" alt="" width=40>
                        Projects
                      </a>
                  </li>
									<li>
											<a href="CV_full_simple_public.html">CV</a>
									</li>
									<!--
									<li>
											<a href="#">Research</a>
									</li>
									-->
									<li>
											<a href="Papers.html">Papers</a>
									</li>
									<li>
											<a href="Activities.html">Activities</a>
									</li>
									<li>
											<a href="Students.html">Students</a>
									</li>
							</ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

<div class="container">
<div class="row">
<div class="col-md-8">

<h2>Projects</h2>

<hr>
<div>
  <h1>Melanie Jutras<img src="images/jutras.jpg" height="75" align="left"></h1>

  <h3>Data Science MS-Thesis Student
  </h3>

  <h3><left>Robust Principal Component Analysis <br>for Anomaly Detection in Cyber Network Data</left></h3>

  <p>  </p>
  <p><img src="images/jutrasMatrix.JPG" height="175" width="400"align="right">The goal of this research is to utilize Robust Principal Component Analysis (RPCA) for the purpose of anomaly detection in DNS network packet data. Computer network traffic meets all of the criteria for Big Data. When dealing with high dimensional data, Principal Component Analysis (PCA) is a common technique used for dimensionality reduction. Because traditional PCA is known to be sensitive to outliers, a robust version of PCA called RPCA is used. Through the use of a tuning parameter, RPCA can be used to separate the original data into two parts: regular network data and anomalous network data. This data science technique allows for tuning a model utilizing a very small amount of training data.  The method described here is useful for cybersecurity because these types of problems are largely unsupervised and often involve high dimensional network data with sparse anomalies.
  </p>

  <h3><left>Publications:</left></h3>
  <A href="paper/jutras_Paper1Title.pdf">Click Here for Paper</A>
</div>

<hr>
<div>
  <h1>Wenjing Li<img src="images\Wenjing_Li.png" height="75" align="left"></h1>

  <h3>Mathematical Department PHD Student
  </h3>

  <h3><left>Large Scale Ensemble Learning <br></left></h3>

  <p>  </p>
  <p><img src="images\ensemble_learning.png" height="250" width="250"align="right">Ensemble learning is a very important technic in machine learning, and it is also widely used in industrial field because of the good performance on predictions and classifications. However, the reason why it works so good is rarely understood. This project aims to figure out the mysterious parts of ensemble learning and even develop some theorems that support ensemble learning. Generally speaking, ensemble learning is a process by which multiple models, such as classifiers or experts, are strategically generated and combined to produce a better algorithm. It is primarily used to improve the classification or prediction performance of a model, or to reduce the risk of selecting a poor model. Therefore, there are two major focuses of ensemble learning: the first one is the process of model generation and selection, and the other one is the model combining strategy.
  </p>

  <h3><left>Publications:</left></h3>
  <A href="papers\Wenjing_Paper1Title.pdf">Click Here for Paper</A>
</div>

<hr>
<div>
  <h1>Haitao Liu<img src="images/haitao.jpeg" height="75" align="left"></h1>

  <h3>Data Science PhD Student
  </h3>

  <h3><left>Robust Graphical Lasso <br>Anomaly Detection</left></h3>

  <p>  </p>
  <p><img src="images/Sparse.png" height="175" width="400"align="right">Gaussian graphical model is
    widely used to study the network structure. However, the standard gaussian graphical
    model is very sensitive to outliers. The existing outliers or anomalies can result
    in a dense information matrix (dense network structure) rather than a desired sparse
    information matrix. So there is a need to develop a robust procedure to filter out the
    anomalies and identify a sparse graphical structure. In this paper, we take the
    advantage of Robust Principle Analysis to add a l1 penalty of a sparse anomaly matrix
    to graphical lasso, and subject to the constraint that the summation of the input
    covariance of the sparse anomaly matrix equals to the observed sample covariance matrix.
     We also propose an ADMM solution to detect the anomaly. The advantage of the algorithm
     is that the ADMM solution can handle large scale data efficiently.
  </p>

  <h3><left>Publications:</left></h3>
  <A href="papers/haitao_Paper1Title.pdf">Click Here for Paper</A>
</div>

<hr>
<div>
  <h1> Yingnan Liu<img src="images/Yingnan.jpg" height="125" align="left"></h1>

  <h3> Data Science PhD Student
  </h3>
  <BR />

  <p><h3>Research</h3>
  </p>
  <h4> Deep Kalman Filter for Non-Gaussian Data - An application with LSTM for Financial Data</h4>

  <p style="width:800px;"> Kalman filter is an estimation technique based on Bayesian Statistics. It is widely used for financial time series analysis.
  However, the filter yields accurate estimation only in special cases of linear data and Gaussian noise. In this research, we focus on
  the extension for nonlinear data and non-Gaussian noises. We combine the filter with Long Short Term Memory networks (LSTM).
  LSTM is used as a mapping function from non-Gaussian space to Gaussian space. We input the mapped space to the filter.
  Then the residual from Kalman Filter is passed back to LSTM for training. </p>


  <p><h3>Education</h3>
  </p>
  <p> M.S. Worcester Polytechnin Institute, Applied Statistics, 2016 </p>
  <p> B.S. University of Waterloo, Mathematical Physics, 2013</p>
</div>

<hr>
<div>
  <h1>Xiaozhou (Joe) Zou<img src="images/Joe.PNG" height="100" width-"100 align="left"></h1>

  <h3>Data Science MS-Thesis Student
  </h3>

  <h3><left>Investigation on Using Randomness Technique to Improve GAN and Mechanism of Batch Normalization</left></h3>

  <p>  </p>
  <p><img src="images/topicImage.PNG" height="400" width="400" align="right">This research is done to better understand Generative Adversarial Networks(GAN). GAN is a deep learning architecture that trains two neural networks simultaneously. Among the two networks, one is called generator and the other is called discriminator. The generator samples from random noise and generate meaningful data, be it image of sound. The discriminator takes as input real data and fake data generated by the generator and tries to distinguish them as well as it can. By competing with each other, both generator and discriminator become better. This architecture is notorious for its instability. It is easy for this architecture to collapse, which means to generate meaningless results. The two goals in this research are both related to the collapse problem of GAN. The first goal is to improve the performance of Generative Adversarial Networks using randomness technique. That is, we hope to improve the convergence rate or the quality of the result via introducing randomness to the training process. Another topic is to understand why batch normalization works so well in solving the collapse problem. To reach this goal, I decompose the batch normalization and try to block some components of batch normalization. I also try to replace the Z-score transformation, which is originally used in batch normalization, by the min-max normalization. This research is still in progress and more results will be added later on.
  </p>

  <h3><left>Publications:</left></h3>
  <A href="papers/joe_Paper_Title.pdf">Click Here for Paper</A>
</div>

</div>
</div>
</div>

</body>
</html>
