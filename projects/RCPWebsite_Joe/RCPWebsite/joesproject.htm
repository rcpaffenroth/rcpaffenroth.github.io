<!DOCTYPE html>
<html>
<body>

<h1>Xiaozhou (Joe) Zou<img src="Joe.PNG" height="100" width-"100 align="left"></h1>

<h3>Data Science MS-Thesis Student 
</h3>

<h3><left>Investigation on Using Randomness Technique to Improve GAN and Mechanism of Batch Normalization</left></h3>

<p>  </p>
<p><img src="topicImage.PNG" height="400" width="400" align="right">This research is done to better understand Generative Adversarial Networks(GAN). GAN is a deep learning architecture that trains two neural networks simultaneously. Among the two networks, one is called generator and the other is called discriminator. The generator samples from random noise and generate meaningful data, be it image of sound. The discriminator takes as input real data and fake data generated by the generator and tries to distinguish them as well as it can. By competing with each other, both generator and discriminator become better. This architecture is notorious for its instability. It is easy for this architecture to collapse, which means to generate meaningless results. The two goals in this research are both related to the collapse problem of GAN. The first goal is to improve the performance of Generative Adversarial Networks using randomness technique. That is, we hope to improve the convergence rate or the quality of the result via introducing randomness to the training process. Another topic is to understand why batch normalization works so well in solving the collapse problem. To reach this goal, I decompose the batch normalization and try to block some components of batch normalization. I also try to replace the Z-score transformation, which is originally used in batch normalization, by the min-max normalization. This research is still in progress and more results will be added later on.
</p>

<h3><left>Publications:</left></h3>
<A href="joe_Paper_Title.pdf">Click Here for Paper</A>


</body>
</html>